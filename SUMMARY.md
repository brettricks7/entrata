# System Summary

This MVP is a single-page React/TypeScript frontend backed by an Express/TypeScript API. The backend integrates with Google Gemini to generate 5-question multiple-choice quizzes for a given topic. We enforce a strict JSON schema with zod and implement deterministic repair to handle malformed LLM outputs. To avoid rate limits and enable offline demos, we cache validated quizzes to the filesystem (keyed by topic/model/version) and support an explicit offline mode. Secrets are handled via a password-locked vault: the Gemini API key is stored encrypted (scrypt + AES‑256‑GCM) and decrypted into memory on demand with a 2-hour TTL auto-expiry; unlock/relock endpoints and a lightweight UI flow manage this lifecycle. We also append raw LLM responses (initial and repair) to an NDJSON log for debugging.

We use Google’s `@google/generative-ai` SDK (Gemini) because it provides strong price/performance for fast JSON-style quiz generation. We request JSON responses and validate against a strict contract to reduce brittleness. Model selection is configurable (`LLM_MODEL`), and we include pragmatic fallbacks to handle API/version differences. This approach balances speed (flash-class models), correctness (schema validation/repair), and operability (filesystem cache, vault-based key handling, and request logging) while keeping the codebase modular and easy to extend (e.g., retrieval grounding, persistence of attempts, richer UI). Caching was chosen to ensure repeatable demos and to mitigate rate limiting/cost. When different questions are desired, the same cache mechanism can still be used by varying the cache key (e.g., adding a `variant` or `seed` parameter, difficulty level, or a `refresh=true` flag) so that fresh quizzes are generated and stored under distinct keys while preserving deterministic reuse for each variant.
